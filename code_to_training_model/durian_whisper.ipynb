{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Pw6bKr1sbG9G"},"outputs":[],"source":["!pip install --force-reinstall --no-deps \"tensorboard==2.19.0\" \"pyarrow>=14.0.0,<20.0.0\""]},{"cell_type":"code","source":["!pip install --upgrade --no-deps datasets[audio]==3.6.0 transformers accelerate evaluate jiwer tensorboard gradio"],"metadata":{"collapsed":true,"id":"sMXMQNf0ba59"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset, DatasetDict, Audio\n","from transformers import (\n","    WhisperTokenizer,\n","    WhisperProcessor,\n","    WhisperFeatureExtractor,\n","    WhisperForConditionalGeneration,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer\n",")\n","from dataclasses import dataclass\n","from typing import Any, Dict, List, Union\n","import torch\n","import evaluate"],"metadata":{"id":"g2XeBH_obhA_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_id = 'openai/whisper-base'\n","out_dir = 'whisper_durian' # Thu mục lưu toàn bộ checkpoint và log sau khi fine-tune\n","epochs = 3\n","batch_size = 32 # Số lượng mẫu audio xử lí cùng lúc trên cùng 1 bước huấn luyện"],"metadata":{"id":"mF7fcM_7b1Pb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"p2PD7Uz0aXsi"}},{"cell_type":"code","source":["atc_dataset_train = load_dataset('quochuy6543/durian_audio_updated_v', split='train')\n","atc_dataset_valid = load_dataset('quochuy6543/durian_audio_updated_v', split='validation')"],"metadata":{"collapsed":true,"id":"8yegnQ3Qb7A4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_extractor = WhisperFeatureExtractor.from_pretrained(model_id)\n","tokenizer = WhisperTokenizer.from_pretrained(model_id, language='vi', task='transcribe')\n","processor = WhisperProcessor.from_pretrained(model_id, language='vi', task='transcribe')"],"metadata":{"collapsed":true,"id":"Vat2jdAScoIR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["atc_dataset_train = atc_dataset_train.cast_column('path', Audio(sampling_rate=16000))\n","atc_dataset_valid = atc_dataset_valid.cast_column('path', Audio(sampling_rate=16000))"],"metadata":{"id":"i25eLV2jcr22"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_dataset(batch):\n","    audio = batch['path']\n","    batch['input_features'] = feature_extractor(audio['array'], sampling_rate=audio['sampling_rate']).input_features[0]\n","    batch['labels'] = tokenizer(batch['text']).input_ids\n","    return batch\n","\n","atc_dataset_train = atc_dataset_train.map(\n","    prepare_dataset,\n","    num_proc=1\n",")\n","\n","atc_dataset_valid = atc_dataset_valid.map(\n","    prepare_dataset,\n","    num_proc=1\n",")"],"metadata":{"id":"_Z57MX20cvgp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@dataclass\n","class DataCollatorSpeechSeq2SeqWithPadding:\n","    processor: Any\n","    decoder_start_token_id: int\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        input_features = [{'input_features': feature['input_features']} for feature in features]\n","        batch = self.processor.feature_extractor.pad(input_features, return_tensors='pt')\n","\n","        label_features = [{'input_ids': feature['labels']} for feature in features]\n","        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors='pt')\n","\n","        labels = labels_batch['input_ids'].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","\n","        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n","            labels = labels[:, 1:]\n","\n","        batch['labels'] = labels\n","\n","        return batch"],"metadata":{"id":"J5olFM1riJO6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = WhisperForConditionalGeneration.from_pretrained(model_id)\n","model.generation_config.task = 'transcribe'\n","model.generation_config.forced_decoder_ids = None"],"metadata":{"id":"iWRNklHfiMj6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n","    processor=processor,\n","    decoder_start_token_id=model.config.decoder_start_token_id,\n",")"],"metadata":{"id":"I4aBWK24ig-U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install jiwer"],"metadata":{"collapsed":true,"id":"LUwITKw4io4D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metric = evaluate.load('wer')\n","def compute_metrics(pred):\n","    pred_ids = pred.predictions\n","    label_ids = pred.label_ids\n","\n","    # replace -100 with the pad_token_id\n","    label_ids[label_ids == -100] = tokenizer.pad_token_id\n","\n","    # we do not want to group tokens when computing the metrics\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n","\n","    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n","\n","    return {'wer': wer}"],"metadata":{"id":"Il594WIGilWt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = Seq2SeqTrainingArguments(\n","    output_dir=out_dir,\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    gradient_accumulation_steps=8,\n","    learning_rate=0.00001,\n","    warmup_steps=1000,\n","    bf16=False,\n","    fp16=True,\n","    num_train_epochs=epochs,\n","    eval_strategy='epoch',\n","    logging_strategy='epoch',\n","    save_strategy='epoch',\n","    predict_with_generate=True,\n","    generation_max_length=225,\n","    report_to=['tensorboard'],\n","    load_best_model_at_end=True,\n","    metric_for_best_model='wer',\n","    greater_is_better=False,\n","    dataloader_num_workers=8,\n","    save_total_limit=2,\n","    lr_scheduler_type='constant',\n","    seed=42,\n","    data_seed=42\n",")"],"metadata":{"id":"I6FzFXP3ixqx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = Seq2SeqTrainer(\n","    args=training_args,\n","    model=model,\n","    train_dataset=atc_dataset_train,\n","    eval_dataset=atc_dataset_valid,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=processor.feature_extractor,\n",")"],"metadata":{"id":"JXul7EiZi00k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()\n","trainer.save_model()\n","processor.save_pretrained(out_dir)"],"metadata":{"id":"MKnHgL5Vi4fp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Trong Colab, sau khi train xong:\n","import shutil\n","\n","# Zip toàn bộ folder model\n","shutil.make_archive('whisper_durian', 'zip', out_dir)\n","\n","# Download file zip về máy\n","from google.colab import files\n","files.download('whisper_durian.zip')"],"metadata":{"id":"kUN_lZoIi5c1"},"execution_count":null,"outputs":[]}]}